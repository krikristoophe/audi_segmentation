{
 "cells": [
  {
   "source": [
    "# Segmentation d'image\n",
    "\n",
    "* Christophe SONNEVILLE\n",
    "* Pierre Leguen\n",
    "\n",
    "Nous avons choisi d'utiliser le jeu de donnée fournis par Audi. Ces données très completes sont composées :\n",
    "- Des images des caméras présentes autour du véhicule\n",
    "- La segmentation faite sur ces images\n",
    "- Des Lidar\n",
    "- Des signaux du bus CAN\n",
    "\n",
    "Nous avons choisi d'utiliser les images des caméras avant et leurs segmentation associé afin de créer un modèle capable de reproduire cette segmentation sur de nouvelles images.\n",
    "\n",
    "Les données peuvent être trouvé sur le site web d'Audi [https://www.a2d2.audi/a2d2/en/dataset.html](https://www.a2d2.audi/a2d2/en/dataset.html)\n",
    "\n",
    "Le jeu de donnée étant très lourd, nous devons d'abord le réduire pour pouvoir l'utiliser\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement et préparation (complet)\n",
    "\n",
    "Dans un premier temps nous avons choisi d'utiliser AWS pour télécharger les +250Go de données et les réduire. Les données on été récupérées et extrait à partir d'un stockage S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tarfile\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadDataset(url):\n",
    "    target_path = url.split('/')[-1]\n",
    "    out_dir = target_path.split('.')[0]\n",
    "    s3.download_file('aev-autonomous-driving-dataset', target_path, target_path)\n",
    "    with tarfile.open(target_path, 'r') as tar:\n",
    "            tar.extractall(path=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Commenter cette ligne pour forcer le téléchargement des données, sinon cette cellule est ignoré\n",
    "# /!\\ Télécharge et extrait 300Go de données automatiquement depuis un stockage AWS\n",
    "downloadDataset('https://aev-autonomous-driving-dataset.s3.eu-central-1.amazonaws.com/camera_lidar_semantic.tar')"
   ]
  },
  {
   "source": [
    "Une partie des données ont été supprimé afin d'alèger le dataset et le rendre utilisable\n",
    "\n",
    "Seul une partie des données de la caméra avant ont été conservées\n",
    "\n",
    "Nous avons gardé environ 20K de données labellisé (40K de photo jpeg au total) pour environ 7.5Go"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertion des png en jpg\n",
    "\n",
    "Audi fournis les images du dataset au format PNG. Nous avons décidé avant tout de convertir ces photos au format JPEG afin de limiter le taille des données. Cette conversion permet de diviser par 10 la taille du dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Commenter cette ligne pour éxecuter cette cellule, sinon elle est ignoré\n",
    "dirs = os.listdir('dataset/png/')\n",
    "index = 0\n",
    "files_len = len(dirs)\n",
    "for file in dirs:\n",
    "    filename = file.split('.')[0]\n",
    "    clear_output(wait=True)\n",
    "    print(\"{}/{}\".format(index, files_len))\n",
    "    index += 1\n",
    "    im = Image.open(\"dataset/png/{}\".format(file))\n",
    "    im.convert('RGB').save(\"dataset/jpg/{}.jpg\".format(filename), 'JPEG')"
   ]
  },
  {
   "source": [
    "## Téléchargement (light)\n",
    "\n",
    "Nous avons mit en ligne sur github le dataset simplifié et allègé\n",
    "\n",
    "Le téléchargement, avec une fibre, prend entre 30 et 45 minutes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Commenter cette ligne pour forcer le téléchargement des données, sinon cette cellule est ignoré\n",
    "!git clone https://github.com/krikristoophe/audi_ds.git dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction IA deep learning\n",
    "\n",
    "Après plusieurs recherches nous avons choisi de créer une modèle deep learning, réputé pour ses performances dans ce type d'application. Tensorflow est une plateform open source permettant de simplifier la création de ce type d'algorithme.\n",
    "\n",
    "Nous nous sommes inspiré et avons adapté les exemples suivants à notre problème :\n",
    "- [https://www.tensorflow.org/tutorials/images/segmentation](https://www.tensorflow.org/tutorials/images/segmentation)\n",
    "- [https://yann-leguilly.gitlab.io/post/2019-12-14-tensorflow-tfdata-segmentation/](https://yann-leguilly.gitlab.io/post/2019-12-14-tensorflow-tfdata-segmentation/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports des différentes fonctions de tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "from IPython.display import clear_output # Permet de vider la console\n",
    "import matplotlib.pyplot as plt # Permet d'afficher des images et des graphiques\n",
    "import pathlib\n",
    "\n",
    "# PIL aide à la gestion des images\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import json\n"
   ]
  },
  {
   "source": [
    "### Lecture des classes\n",
    "\n",
    "Le fichier `class_list.json` contient l'association des classes à chaque couleurs présente dans les masques fournis par Audi"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(255, 0, 0), (200, 0, 0), (150, 0, 0), (128, 0, 0), (182, 89, 6), (150, 50, 4), (90, 30, 1), (90, 30, 30), (204, 153, 255), (189, 73, 155), (239, 89, 191), (255, 128, 0), (200, 128, 0), (150, 128, 0), (0, 255, 0), (0, 200, 0), (0, 150, 0), (0, 128, 255), (30, 28, 158), (60, 28, 100), (0, 255, 255), (30, 220, 220), (60, 157, 199), (255, 255, 0), (255, 255, 200), (233, 100, 0), (110, 110, 0), (128, 128, 0), (255, 193, 37), (64, 0, 64), (185, 122, 87), (0, 0, 100), (139, 99, 108), (210, 50, 115), (255, 0, 128), (255, 246, 143), (150, 0, 150), (204, 255, 153), (238, 162, 173), (33, 44, 177), (180, 50, 180), (255, 70, 185), (238, 233, 191), (147, 253, 194), (150, 150, 200), (180, 150, 200), (72, 209, 204), (200, 125, 210), (159, 121, 238), (128, 0, 255), (255, 0, 255), (135, 206, 255), (241, 230, 255), (96, 69, 143), (53, 46, 82)]\n"
     ]
    }
   ],
   "source": [
    "with open('class_list.json', 'r') as f:\n",
    "    class_list = json.load(f)\n",
    "    class_colors = list(class_list.keys())\n",
    "    class_colors = [PIL.ImageColor.getcolor(color, 'RGB') for color in class_colors]\n",
    "    class_colors_index = np.asarray(class_colors)\n",
    "    lass_colors_index = class_colors_index.astype(np.uint8)\n",
    "    print(class_colors) # Liste des couleurs au format (R, G, B)"
   ]
  },
  {
   "source": [
    "### Correction des couleurs alteré par le stockage en JPEG\n",
    "\n",
    "Dans le but de limiter le taille du jeu de donnée, nous avons converti toutes les images au format JPEG. Cette conversion divise par 10 le stockage nécessaire mais altère les couleurs des images.\n",
    "\n",
    "Cette altération ne pose pas de problème pour les photos prises mais sont problématiques pour les masques. L'association entre les couleurs et les classes ne pouvant plus se faire correctement, nous avons choisi de corriger ces couleurs en temps réel.\n",
    "\n",
    "Pour cela, lors du prétraitement des données, chaque pixel du masque sera remplacé par la couleur éxistant dans la liste des classes la plus proche de celle stocké.\n",
    "\n",
    "Ex: si dans le masque un pixel à une couleur RBG à `(254, 2, 1)`, celle ci sera remplacé par `(255, 0, 0)`\n",
    "\n",
    "Source: [https://stackoverflow.com/a/9018153/10367233](https://stackoverflow.com/a/9018153/10367233)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d=sqrt((r2-r1)^2+(g2-g1)^2+(b2-b1)^2)\n",
    "\n",
    "\n",
    "# Calcule la distance entre 2 couleurs RGB\n",
    "def get_distance_color_rgb(a, b):\n",
    "    s = 0\n",
    "    for i in range(3):\n",
    "        s += ((b[i] - a[i]) ** 2)\n",
    "    return math.sqrt(s)\n",
    "\n",
    "# Renvoi la couleur de masque la plus proche\n",
    "def get_nearest_color(c):\n",
    "    best = None\n",
    "    best_dist = None\n",
    "    for color in class_colors:\n",
    "        if c == color:\n",
    "            return color\n",
    "        d = get_distance_color_rgb(c, color)\n",
    "        if best is None or d < best_dist:\n",
    "            best_dist = d\n",
    "            best = color\n",
    "    return best\n",
    "\n",
    "# Renvoi la couleur de masque la plus proche avec une sauvegarde en cache\n",
    "# Le cache permet de limiter le temps de calcule en sauvegardant les couleurs déja associées\n",
    "color_cache = dict()\n",
    "def gnc_cache(c):\n",
    "    k = \"{}{}{}\".format(c[0], c[1], c[2])\n",
    "    if k in color_cache:\n",
    "        return color_cache[k]\n",
    "    new_color = get_nearest_color(tuple(c))\n",
    "    color_cache[k] = new_color\n",
    "    return new_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le remplacement des pixels via numpy permet d'accèlerer le traitement sur beaucoup de pixels\n",
    "def manage_pixel(pix):\n",
    "    return np.array(gnc_cache(pix))\n",
    "\n",
    "# Remplace tout les pixels par la couleur du masque la plus proche\n",
    "def manage_np_img(np_img):\n",
    "    return np.apply_along_axis(manage_pixel, 2, np_img)"
   ]
  },
  {
   "source": [
    "### Définition des variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=128 # taille des images traité\n",
    "IMG_W=IMG_SIZE\n",
    "IMG_H=IMG_SIZE\n",
    "N_CHANNELS=3 # 3 couleurs RGB\n",
    "N_CLASSES=N_CLASSES=len(class_colors_index) ## 55 normalement\n",
    "input_size = (IMG_W, IMG_H, N_CHANNELS) # format de l'input du modèle\n",
    "BATCH_SIZE = 32 # Nombre de donnée traité \"d'un coup\"\n",
    "BUFFER_SIZE = 20\n",
    "SEED=123\n",
    "EPOCHS = 2 # Nombre de passage par entrainement"
   ]
  },
  {
   "source": [
    "### Gestion de l'affichage"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse le processus rgb->classe\n",
    "def class_to_rgb(incoming):\n",
    "    palette = class_colors_index\n",
    "    W, H, _ = incoming.shape\n",
    "    palette = tf.constant(palette, dtype=tf.uint8)\n",
    "    class_indexes = tf.reshape(incoming, [-1])\n",
    "    color_image = tf.gather(palette, class_indexes)\n",
    "    color_image = tf.reshape(color_image, [W, H, 3])\n",
    "\n",
    "    color_image = tf.cast(color_image, dtype=tf.float32)\n",
    "    return color_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affiche une lise d'image (l'input, le masque réel et le masque prédit)\n",
    "def display_sample(display_list):\n",
    "    plt.figure(figsize=(18, 18))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "### Préparation et prétraitement des données"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Préparation en temps réel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupère et décode les images\n",
    "def parse_image_name(filename, channels=3):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_jpeg(img, channels=channels)\n",
    "    img = tf.image.convert_image_dtype(img, tf.uint8)\n",
    "    return img\n",
    "\n",
    "# Récupère l'image d'input et le masque correspondant\n",
    "def parse_image(filename):\n",
    "    cam = filename\n",
    "    label = tf.strings.regex_replace(filename, \"/val\", \"\")\n",
    "    label = tf.strings.regex_replace(label, \"/train\", \"\")\n",
    "    label = tf.strings.regex_replace(label, \"camera\", \"label\")\n",
    "    label = tf.strings.regex_replace(label, \"cam\", \"label\")\n",
    "    cam_img = parse_image_name(cam)\n",
    "    label_img = parse_image_name(label)\n",
    "    return dict(image=cam_img, mask=label_img, filename=filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation des données\n",
    "@tf.function\n",
    "def normalize(input_image, input_mask):\n",
    "    # Transformation de l'input de int[0, 255] à float[0, 1]\n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "\n",
    "    # Correction des couleurs du masque\n",
    "    im = tf.numpy_function(manage_np_img, [input_mask], tf.int64)\n",
    "    input_mask = tf.reshape(im, input_mask.shape)\n",
    "    input_mask = tf.cast(input_mask, tf.uint8)\n",
    "    \n",
    "    # Transformation des couleurs en classes (index de color_classes_index)\n",
    "    # https://www.spacefish.biz/2020/11/rgb-segmentation-masks-to-classes-in-tensorflow/\n",
    "    one_hot_map = []\n",
    "    for color in class_colors_index:\n",
    "        class_map = tf.reduce_all(tf.equal(input_mask, color), axis=-1)\n",
    "        one_hot_map.append(class_map)\n",
    "    one_hot_map = tf.stack(one_hot_map, axis=-1)\n",
    "    one_hot_map = tf.cast(one_hot_map, tf.float32)\n",
    "    input_mask = tf.argmax(one_hot_map, axis=-1)\n",
    "    input_mask = tf.expand_dims(input_mask, axis=-1)\n",
    "    return input_image, input_mask\n",
    "\n",
    "# Redimentionnement et normalisation des données d'entrainement\n",
    "@tf.function\n",
    "def load_image_train(datapoint):\n",
    "    input_image = tf.image.resize(datapoint['image'], (IMG_W, IMG_H))\n",
    "    input_mask = tf.image.resize(datapoint['mask'], (IMG_W, IMG_H))\n",
    "\n",
    "    # Retournement aléatoire de l'image pour plus de diversité\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        input_mask = tf.image.flip_left_right(input_mask)\n",
    "\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "    return input_image, input_mask\n",
    "\n",
    "# Redimentionnement et normalisation des données de validation\n",
    "@tf.function\n",
    "def load_image_test(datapoint):\n",
    "    input_image = tf.image.resize(datapoint['image'], (IMG_W, IMG_H))\n",
    "    input_mask = tf.image.resize(datapoint['mask'], (IMG_W, IMG_H))\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement des données d'entraiement\n",
    "# Chargement, mélange aléatoir, répétitions\n",
    "def preprocess_train(t_ds):\n",
    "    t_ds = t_ds.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    t_ds = t_ds.shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "    t_ds = t_ds.repeat(EPOCHS)\n",
    "    t_ds = t_ds.batch(BATCH_SIZE)\n",
    "    t_ds = t_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return t_ds\n",
    "\n",
    "# Prétraitement des données de validation\n",
    "# Chargement, mélange aléatoir, répétitions\n",
    "def preprocess_val(v_ds):\n",
    "    v_ds = v_ds.map(load_image_test)\n",
    "    v_ds = v_ds.repeat(EPOCHS)\n",
    "    v_ds = v_ds.batch(BATCH_SIZE)\n",
    "    v_ds = v_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return v_ds"
   ]
  },
  {
   "source": [
    "#### Création des jeux de données"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un dataset\n",
    "# max_ds permet de limiter la taille du dataset (utile pour les tests)\n",
    "def create_dataset(max_ds=None):\n",
    "    # Trouve le dataset entier - shuffle\n",
    "    full_ds = tf.data.Dataset.list_files(\"./dataset/cam/**/*.jpg\", seed=123).shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "\n",
    "    # Sépare le dataset - permet de faire des tests sur moins de données\n",
    "    if max_ds is not None:\n",
    "        full_ds = full_ds.take(max_ds)\n",
    "\n",
    "    # Séparation des données retenues en dataset d'entrainement et de validation\n",
    "    ds_train = full_ds.take(round(len(full_ds)*0.9))\n",
    "    ds_val = full_ds.skip(len(ds_train))\n",
    "\n",
    "    # lecture des images\n",
    "    ds_train = ds_train.map(parse_image)\n",
    "    ds_val = ds_val.map(parse_image)\n",
    "    dataset = {\n",
    "        \"train\": preprocess_train(ds_train),\n",
    "        \"val\": preprocess_val(ds_val)\n",
    "    }\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<PrefetchDataset shapes: ((None, 128, 128, 3), (None, 128, 128, 1)), types: (tf.float32, tf.int64)>\n<PrefetchDataset shapes: ((None, 128, 128, 3), (None, 128, 128, 1)), types: (tf.float32, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = create_dataset()\n",
    "\n",
    "print(dataset['train'])\n",
    "print(dataset['val'])"
   ]
  },
  {
   "source": [
    "La validation croisée permet d'évaluer un algorithme en créant plusieurs modèle entrainé à partir d'un jeu de donnée divisé en plusieurs groupes. Les variables d'évaluation de ces modèles (loss ou accuracy) sont ensuite moyenné.\n",
    "\n",
    "Pour cela plusieurs dataset sont généré à partir du dataset principal en suivant ce schéma:\n",
    "\n",
    "![](https://user.oc-static.com/upload/2017/02/27/14881889982452_P1C2-2.png)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de plusieurs dataset pour la validation croisé\n",
    "# n_group doit être supérieur à 1 (au moins 1 groupe de test et 1 groupe de validation)\n",
    "# max_ds permet de limiter la taille du dataset (utile pour les tests)\n",
    "# Chaque groupe doit contenir au moins BATCH_SIZE de données\n",
    "def create_cv_dataset(n_group, max_ds=None):\n",
    "    # Trouve le dataset entier - shuffle\n",
    "    f_ds = tf.data.Dataset.list_files(\"./dataset/cam/**/*.jpg\", seed=123).shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "    if max_ds is not None:\n",
    "        full_ds = f_ds.take(max_ds)\n",
    "    else:\n",
    "        full_ds = f_ds\n",
    "\n",
    "    # Le dataset complet ou partiel sera divisé en n_group de taille group_size\n",
    "    group_size = math.floor(len(full_ds) / n_group)\n",
    "    cv_ds = list()\n",
    "\n",
    "    for ig in range(n_group):\n",
    "        left_size = group_size*(ig)\n",
    "        right_size = len(full_ds) - left_size - group_size\n",
    "        left_ds = full_ds.take(left_size)\n",
    "        val_ds = full_ds.skip(left_size).take(group_size)\n",
    "        right_ds = full_ds.skip(left_size + group_size).take(right_size)\n",
    "        train_ds = left_ds.concatenate(right_ds)\n",
    "        \n",
    "        train_ds = train_ds.map(parse_image)\n",
    "        val_ds = val_ds.map(parse_image)\n",
    "        c_ds = {\n",
    "            \"train\": preprocess_train(train_ds),\n",
    "            \"val\": preprocess_val(val_ds)\n",
    "        }\n",
    "        cv_ds.append(c_ds)\n",
    "    return cv_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_group = 3\n"
     ]
    }
   ],
   "source": [
    "cv_dataset = create_cv_dataset(3, 500)\n",
    "print(\"n_group = {}\".format(len(cv_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Cet affichage peut prendre du temps, commenter cette ligne pour éxecuter la cellule\n",
    "# Affichage d'une donnée et son label correspondant\n",
    "for image, mask in dataset['train'].take(1):\n",
    "    si, sm = image, mask\n",
    "display_sample([si[0], class_to_rgb(sm[0])])"
   ]
  },
  {
   "source": [
    "### Définition du modèle\n",
    "\n",
    "Pour ce type d'application les algorithmes encodeur-décodeur sont réputé comme étant les plus performants.\n",
    "\n",
    "![](https://i.stack.imgur.com/1sccx.png)\n",
    "\n",
    "Le principe de ces algorithmes est de diviser le travail en 2 parties:\n",
    "- L'encodeur extrait les informations (traits, formes, couleurs etc) d'une donnée (ici notre image)\n",
    "- Le décodeur utilse les informations de l'encodeur pour générer une sortie (ici un masque de segmentation)\n",
    "\n",
    "Les images ont (dans la plupart des cas) toujours les mêmes informations à extraires\n",
    "\n",
    "Ex: une ligne droite dans une photo d'arbre n'as pas de différence avec une ligne droite d'une photo de chat\n",
    "\n",
    "Nous n'avons donc pas d'intérêt à entrainer un encodeur. Des modèles déja entrainé existe et sont disponible gratuitement. Seul le décodeur change en fonction des applications. Celui ci sera donc entrainé à partir de nos données.\n",
    "\n",
    "N'ayant pas les ressources nécessaire (GPUs nottement) pour entrainer des modèles deep learning et tester plusieurs algorithmes de décodeur, nous avons choisi de modifier l'encodeur afin d'évaluer les performances globales. Nous évaluerons donc les performances des algorithmes avec MobileNetV2 et MobileNetV3\n",
    "\n",
    "L'agorithme du décodeur est donc celui présenté dans le tutoriel de tensorflow avec lequel nous avons eu des résultats satisfaisant:\n",
    "\n",
    "[https://www.tensorflow.org/tutorials/images/segmentation#define_the_model](https://www.tensorflow.org/tutorials/images/segmentation#define_the_model)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génère et prépare l'encodeur à partir de MobileNetV2\n",
    "def create_encodeur():\n",
    "  base_model = tf.keras.applications.MobileNetV2(input_shape=input_size, include_top=False)\n",
    "  layer_names = [\n",
    "      'block_1_expand_relu',\n",
    "      'block_3_expand_relu',\n",
    "      'block_6_expand_relu',\n",
    "      'block_13_expand_relu',\n",
    "      'block_16_project',\n",
    "  ]\n",
    "  layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "  # Création du modèle de l'encodeur\n",
    "  down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "  # L'encodeur ne doit pas être entrainé lors de notre phase d'entrainement\n",
    "  down_stack.trainable = False \n",
    "  return down_stack\n",
    "\n",
    "\n",
    "# Génère un modèle encodeur-décodeur\n",
    "def create_global_algo():\n",
    "  up_stack = [\n",
    "    pix2pix.upsample(512, 3),\n",
    "    pix2pix.upsample(256, 3),\n",
    "    pix2pix.upsample(128, 3),\n",
    "    pix2pix.upsample(64, 3),\n",
    "  ]\n",
    "  down_stack = create_encodeur()\n",
    "  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
    "  x = inputs\n",
    "  skips = down_stack(x)\n",
    "  x = skips[-1]\n",
    "  skips = reversed(skips[:-1])\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    x = concat([x, skip])\n",
    "  last = tf.keras.layers.Conv2DTranspose(\n",
    "      N_CLASSES, 3, strides=2,\n",
    "      padding='same')\n",
    "\n",
    "  x = last(x)\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "\n",
    "\n",
    "def create_new_model():\n",
    "  m = create_global_algo()\n",
    "  m.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "  return m\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforme la sortie du modèle en un masque de classes\n",
    "def create_mask(pred_mask: tf.Tensor) -> tf.Tensor:\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = tf.expand_dims(pred_mask, axis=-1)\n",
    "    return pred_mask[0]\n",
    "\n",
    "# Affiche le résultat du modèle pour une donnée précise\n",
    "def show_predictions(dataset=None, num=1):\n",
    "    one_img_batch = sample_image[0][tf.newaxis, ...]\n",
    "    inference = model.predict(one_img_batch)\n",
    "    pred_mask = class_to_rgb(create_mask(inference))\n",
    "    display_sample([sample_image[0], class_to_rgb(sample_mask[0]),\n",
    "                    pred_mask])"
   ]
  },
  {
   "source": [
    "### Entrainement du modèle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sur les cpu, une epoch prend entre 15 et 35 minutes pour 5K de dataset\n",
    "\n",
    "def train_model(m, train_ds, val_ds):\n",
    "    TRAINSET_SIZE = len(train_ds)\n",
    "    VALSET_SIZE = len(val_ds)\n",
    "    STEPS_PER_EPOCH = TRAINSET_SIZE // BATCH_SIZE\n",
    "    VALIDATION_STEPS = VALSET_SIZE // BATCH_SIZE\n",
    "    model_history = m.fit(train_ds, epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=val_ds,\n",
    "                          verbose=1)\n",
    "    return model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # l'entrainement d'un modèle peut prendre du temps, commenter celle ligne pour éxecuter la cellule\n",
    "model_history = train_model(model, dataset['train'], dataset['val'])"
   ]
  },
  {
   "source": [
    "### Affichage des résultats"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error # Cet affichage peut prendre du temps, commenter cette ligne pour éxecuter la cellule\n",
    "for image, mask in dataset['val'].skip(2).take(1):\n",
    "    sample_image, sample_mask = image, mask\n",
    "show_predictions()"
   ]
  },
  {
   "source": [
    "### Sauvegarde du model\n",
    "\n",
    "[https://www.tensorflow.org/tutorials/keras/save_and_load#manually_save_weights](https://www.tensorflow.org/tutorials/keras/save_and_load#manually_save_weights)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_4\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_5 (InputLayer)            [(None, 128, 128, 3) 0                                            \n__________________________________________________________________________________________________\nmodel_3 (Functional)            [(None, 64, 64, 96), 1841984     input_5[0][0]                    \n__________________________________________________________________________________________________\nsequential_4 (Sequential)       (None, 8, 8, 512)    1476608     model_3[0][4]                    \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 8, 8, 1088)   0           sequential_4[0][0]               \n                                                                 model_3[0][3]                    \n__________________________________________________________________________________________________\nsequential_5 (Sequential)       (None, 16, 16, 256)  2507776     concatenate_4[0][0]              \n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, 16, 16, 448)  0           sequential_5[0][0]               \n                                                                 model_3[0][2]                    \n__________________________________________________________________________________________________\nsequential_6 (Sequential)       (None, 32, 32, 128)  516608      concatenate_5[0][0]              \n__________________________________________________________________________________________________\nconcatenate_6 (Concatenate)     (None, 32, 32, 272)  0           sequential_6[0][0]               \n                                                                 model_3[0][1]                    \n__________________________________________________________________________________________________\nsequential_7 (Sequential)       (None, 64, 64, 64)   156928      concatenate_6[0][0]              \n__________________________________________________________________________________________________\nconcatenate_7 (Concatenate)     (None, 64, 64, 160)  0           sequential_7[0][0]               \n                                                                 model_3[0][0]                    \n__________________________________________________________________________________________________\nconv2d_transpose_9 (Conv2DTrans (None, 128, 128, 55) 79255       concatenate_7[0][0]              \n==================================================================================================\nTotal params: 6,579,159\nTrainable params: 6,546,327\nNon-trainable params: 32,832\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('models/m5')\n",
    "new_model.summary()"
   ]
  },
  {
   "source": [
    "### Cross validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "13/13 [==============================] - 124s 9s/step - loss: 3.3186 - accuracy: 0.1996 - val_loss: 1.4243 - val_accuracy: 0.6359\n",
      "Epoch 2/2\n",
      "13/13 [==============================] - 122s 10s/step - loss: 1.3301 - accuracy: 0.7099 - val_loss: 1.0445 - val_accuracy: 0.7680\n",
      "INFO:tensorflow:Assets written to: ./models/cv_m0/assets\n",
      "\n",
      "Epoch 1/2\n",
      "13/13 [==============================] - 133s 10s/step - loss: 2.9712 - accuracy: 0.2571 - val_loss: 1.4166 - val_accuracy: 0.6451\n",
      "Epoch 2/2\n",
      "13/13 [==============================] - 121s 10s/step - loss: 1.2823 - accuracy: 0.7476 - val_loss: 1.0112 - val_accuracy: 0.7670\n",
      "INFO:tensorflow:Assets written to: ./models/cv_m1/assets\n",
      "\n",
      "Epoch 1/2\n",
      "13/13 [==============================] - 129s 10s/step - loss: 3.1233 - accuracy: 0.2659 - val_loss: 1.4410 - val_accuracy: 0.6141\n",
      "Epoch 2/2\n",
      "13/13 [==============================] - 121s 10s/step - loss: 1.3019 - accuracy: 0.7118 - val_loss: 1.0418 - val_accuracy: 0.7787\n",
      "INFO:tensorflow:Assets written to: ./models/cv_m2/assets\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "cv_dataset = create_cv_dataset(3, max_ds=10000)\n",
    "history = list()\n",
    "for i in range(len(cv_dataset)):\n",
    "    cv_ds = cv_dataset[i]\n",
    "    m = create_model()\n",
    "    model_history = train_model(m, cv_ds['train'], cv_ds['val'])\n",
    "    history.append(model_history)\n",
    "    m.save(\"./models/cv_m{}\".format(i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 1.202001969019572, 'val_loss': 1.024643361568451, 'accuracy': 0.7577516039212545, 'val_accuracy': 0.7753309607505798}\n"
     ]
    }
   ],
   "source": [
    "def process_model_history(history):\n",
    "    sums = {\n",
    "        'loss': 0,\n",
    "        'accuracy': 0,\n",
    "        'val_loss': 0,\n",
    "        'val_accuracy': 0\n",
    "    }\n",
    "    for hist in history:\n",
    "        sums['loss'] += hist.history['loss'][-1]\n",
    "        sums['accuracy'] += hist.history['accuracy'][-1]\n",
    "        sums['val_loss'] += hist.history['val_loss'][-1]\n",
    "        sums['val_accuracy'] += hist.history['val_accuracy'][-1]\n",
    "    return {\n",
    "        'loss': sums['loss'] / len(history),\n",
    "        'val_loss': sums['val_loss'] / len(history),\n",
    "        'accuracy': sums['accuracy'] / len(history),\n",
    "        'val_accuracy': sums['val_accuracy'] / len(history),\n",
    "    }\n",
    "\n",
    "cross_val_eval = process_model_history(history)\n",
    "print(cross_val_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 1.2053653399149578, 'val_loss': 1.032489577929179, 'accuracy': 0.7576147715250651, 'val_accuracy': 0.771234412988027}\n"
     ]
    }
   ],
   "source": [
    "cross_val_eval = process_model_history(history)\n",
    "print(cross_val_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aaa743e98f9ad5a1f6fbc27e05147b3fd460e61150fa4336a21434782072747d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}